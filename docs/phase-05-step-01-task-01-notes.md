# Phase 05 · Step 01 · Task 01 – Connect and Inspect `books` Table

## 1. Task definition and goal

Operationalize the plan’s “connect and inspect” checklist: boot the Docker stack, document every `psql` command required to prove the `books` table still matches the raw CSV, and export schema + preview artifacts via a Python CLI so reviewers can verify results without spinning up Postgres. Success means any teammate can copy the commands below, confirm the 11,119-row count, and capture the same evidence inside `outputs/phase05_postgres_validation/`.

## 2. How to run this analysis script

1. **Anchor in the repo root** so Docker bind mounts map to `/app` inside each container:

```powershell
cd C:\Users\shady\Documents\GITHUB\goodreads-books-analytics
```

2. **Start (or restart) the shared Docker stack** every time you open a new terminal session:

```powershell
docker compose -f docker-compose.python.yml -f docker-compose.postgresql.yml up -d
```

Wait until `docker compose ps` shows both `app` and `postgres` containers in the `Up` state before proceeding.

3. **Inspect the database with `psql`**. Keep `PAGER=cat` + `-q` to avoid pager errors and suppress banners:

```powershell
docker compose -f docker-compose.python.yml -f docker-compose.postgresql.yml exec -e PAGER=cat `
   postgres psql -q -U goodreads_user -d goodreads
```

Run the standard checkpoints from the `goodreads=#` prompt:

```psql
\l                            -- confirm the goodreads database exists
\dt                           -- list public tables (books, books_clean, bookid_canonical_map, book_authors_stage)
\d+ books                     -- display the 14-column schema
SELECT COUNT(*) FROM books;    -- expect 11,119 rows
```

4. **Export schema + preview evidence** using the validation CLI. This runs inside the Python container, reuses `.env` credentials via SQLAlchemy, and writes CSV artifacts for version control:

```powershell
docker compose -f docker-compose.python.yml -f docker-compose.postgresql.yml exec app `
   python -m src.analyses.postgres_validate_books --table books --sample-limit 5
```

Optional flags:

- `--order-column ratings_count` to preview the most reviewed titles.
- `--schema analytics` if the table lives outside `public` (validated via `validate_identifier`).

5. **Exit `psql`** with `\q` after collecting outputs. Paste the command transcripts directly into the task note to preserve the audit trail.

## 3. Environment recap

- **Containers:** `app` (Python 3.11 slim with pandas/SQLAlchemy) and `postgres` (PostgreSQL 17) orchestrated via `docker-compose.python.yml` + `docker-compose.postgresql.yml`.
- **Networking:** Containers share the default Compose network; `app` connects to host `postgres:5432`. Local GUIs can use `localhost:5491` per `.env`.
- **Credentials:** `POSTGRES_DB=goodreads`, `POSTGRES_USER=goodreads_user`, `POSTGRES_PASSWORD=<value from .env>`; reused everywhere via `src/db_config.build_database_url_from_env`.
- **Bind mount:** Repo root → `/app` in both containers, enabling `psql` to run `\i sql/...` and CLIs to read/write under `/app/outputs`.
- **Outputs:** `outputs/phase05_postgres_validation/` stores the schema + preview CSVs generated by the validation CLI.

## 4. Findings / results

| Checkpoint         | Evidence                                                                                                                                                                 |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Database inventory | `\dt` lists four public tables (`books`, `books_clean`, `bookid_canonical_map`, `book_authors_stage`) owned by `goodreads_user`.                                         |
| Schema snapshot    | `\d+ books` prints 14 columns (12 Goodreads fields + `authors_raw`, `authors_clean`). CLI exported the same metadata to `books_schema_snapshot.csv`.                     |
| Row count parity   | `SELECT COUNT(*) FROM books;` → **11,119**, matching `pd.read_csv(..., on_bad_lines="skip")`. Confirms Postgres remains in sync with the raw CSV ingestion.              |
| Preview sample     | `books_sample_preview.csv` captures the first five `book_id` rows (Harry Potter titles) with clean text encoding, proving the SQLAlchemy → pandas round-trip is healthy. |
| Logging hygiene    | Using `PAGER=cat` + `-q` eliminates the Alpine `TURN` warning, giving copy/paste-ready transcripts for portfolios.                                                       |

## 5. Expected output checkpoints

- `\l` includes the `goodreads` database owned by `goodreads_user`.
- `\dt` shows the four-table list above.
- `\d+ books` renders a 14-row column table (look for `authors_raw` and `authors_clean`).
- `SELECT COUNT(*) FROM books;` returns `11119`.
- Validation CLI logs:
  - `INFO __main__ - Discovered 14 columns in public.books`
  - `INFO __main__ - Row count for public.books: 11,119`
  - `INFO __main__ - Wrote ... books_schema_snapshot.csv`
  - `INFO __main__ - Wrote ... books_sample_preview.csv`

## 6. Observations / insights

- Persisting `authors_raw` + `authors_clean` in the table gives analysts both the literal Goodreads string and the normalized list used for exploding authors—no need to reparse text in SQL.
- `PAGER=cat` is non-negotiable inside these lightweight containers; forgetting it recreates the cryptic `TURN: No such file or directory` warning every time `psql` wants a pager.
- Reading `data/books.csv` with `engine="python"` + `on_bad_lines="skip"` mirrors the loader’s behavior and keeps the Postgres row count aligned with pandas.
- Exporting CSV evidence beats screenshots: Git diffs make schema drift obvious, and reviewers can inspect the artifacts directly on GitHub.

## 7. Artifacts refreshed each run

- `outputs/phase05_postgres_validation/books_schema_snapshot.csv`
- `outputs/phase05_postgres_validation/books_sample_preview.csv`
- This note (`docs/phase-05-step-01-task-01-notes.md`) with updated command transcripts

## 8. Q&A / data troubleshooting

| Question                                             | Answer                                                                                                                                                                     |
| ---------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| How do I list databases/tables without pager errors? | Include `-e PAGER=cat` in the `psql` command or run `\pset pager off` once inside the shell.                                                                               |
| Where are the schema + preview exports stored?       | `outputs/phase05_postgres_validation/books_schema_snapshot.csv` and `books_sample_preview.csv`. Commit them whenever the schema changes.                                   |
| Can I change the preview sample?                     | Use `--sample-limit` and `--order-column` flags when running `postgres_validate_books` (e.g., `--sample-limit 10 --order-column ratings_count`).                           |
| What if the row count ≠ 11,119?                      | Reload the table (`python -m src.load_books_to_postgres --table books`) and rerun the CLI. If it still diverges, inspect `data/books.csv` for edits or new malformed rows. |
| How do I connect from DBeaver/PGAdmin?               | Use the `.env` values: host `localhost`, port `5491`, db `goodreads`, user `goodreads_user`, password from `.env`. This mirrors the Docker workflow.                       |

## 9. Checklist review (from the plan)

- [x] PostgreSQL stack running via Docker.
- [x] Connected with `psql` and listed databases/tables.
- [x] Inspected `books` schema via `\d+`.
- [x] Verified `books` row count matches the CSV reference (11,119 rows).
- [x] Exported schema + preview artifacts with the Python CLI.
- [x] Documented all commands, outputs, and artifacts.

## 10. Appendices (log excerpts)

```
$ docker compose -f docker-compose.python.yml -f docker-compose.postgresql.yml exec -e PAGER=cat postgres psql -q -U goodreads_user -d goodreads -c "\dt"
             List of relations
 Schema |         Name         | Type  |     Owner
--------+----------------------+-------+------------
 public | book_authors_stage   | table | goodreads_user
 public | bookid_canonical_map | table | goodreads_user
 public | books                | table | goodreads_user
 public | books_clean          | table | goodreads_user

$ docker compose -f docker-compose.python.yml -f docker-compose.postgresql.yml exec app python -m src.analyses.postgres_validate_books --table books --sample-limit 5
2025-12-05 05:22:41,841 INFO __main__ - Discovered 14 columns in public.books
2025-12-05 05:22:41,843 INFO __main__ - Row count for public.books: 11,119
2025-12-05 05:22:41,860 INFO __main__ - Retrieved 5 preview rows from public.books
2025-12-05 05:22:41,876 INFO __main__ - Wrote 14 rows to outputs/phase05_postgres_validation/books_schema_snapshot.csv
2025-12-05 05:22:41,888 INFO __main__ - Wrote 5 rows to outputs/phase05_postgres_validation/books_sample_preview.csv
```

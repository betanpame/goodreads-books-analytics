# Goodreads Data Cleaning Rules (Phase 03 · Step 03 · Task 03)

These rules translate the findings from Phase 03 Tasks 01–02 into concrete, reproducible cleaning steps to be implemented during Phase 04. Each rule notes the affected columns, the decision, rationale, priority, and evidence artifacts.

| #   | Issue & Evidence                                                                                                                                         | Columns                                          | Rule / Decision                                                                                                                                                                      | Rationale                                                                                                 | Priority     |
| --- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------- | ------------ |
| 1   | **Publication year parse failures** – 2 rows missing `publication_year` but retaining raw `publication_date` strings (see `missing_values_summary.csv`). | `publication_year`, `publication_date`           | Re-parse using `pd.to_datetime(..., errors='coerce')` with fallback patterns; if still missing, label `publication_year` as `Unknown` but retain row.                                | Preserves otherwise valid book records and keeps time-series charts consistent.                           | Must-do now  |
| 2   | **Partial duplicates (31 rows)** – repeated `(title, authors, publication_date)` combos documented in `partial_duplicates_by_subset.csv`.                | `title`, `authors`, `publication_date`, `bookID` | Use `data/derived/duplicate_bookid_mapping.csv` to map duplicates to canonical `bookID`. Aggregate analytics and SQL views on `canonical_book_id`; retain original rows for lineage. | Prevents double counting multi-format releases without deleting source data.                              | Must-do now  |
| 3   | **Zero-page audiobook/study-guide records (76 rows)** – `num_pages=0` in `num_pages_below_valid_min.csv`.                                                | `  num_pages`, `format` (to be derived)          | Set `num_pages` to `NaN` and flag `media_type='audio_or_misc'`. Optionally exclude these rows when analyzing print length.                                                           | Zero pages are physically impossible for print; tagging prevents skew while keeping content discoverable. | Must-do now  |
| 4   | **Ultra-short entries (`num_pages < 10`, 195 rows)** – e.g., summaries and guides.                                                                       | `  num_pages`                                    | Keep rows but tag `page_length_bucket='short_reference'`. Review downstream metrics to ensure they are optionally filtered.                                                          | These references can stay but should not skew average page length KPIs.                                   | Nice-to-have |
| 5   | **Multi-volume omnibuses (`num_pages > 2000`, 12 rows)** – e.g., "The Complete Aubrey/Maturin" (6,576 pages).                                            | `  num_pages`                                    | Cap reported pages at 2,000 for summary stats and add `page_length_bucket='multi_volume'`. Retain true value in a raw column for provenance.                                         | Keeps charts readable while still indicating that these are multi-volume works.                           | Must-do now  |
| 6   | **Average rating placeholders (250 rows outside IQR bounds)** – `average_rating = 0` for reference guides; some perfect 5.0 bundles.                     | `average_rating`                                 | Treat `average_rating = 0` as missing; drop from KPI aggregates or impute with category median. Keep legitimate 5.0 scores but flag them for manual review.                          | Prevents placeholder zeros from lowering averages; retains genuine enthusiasm signals.                    | Must-do now  |
| 7   | **Long-tailed engagement metrics (`ratings_count`, `text_reviews_count`)** – 1,729 and 1,631 rows above IQR upper bounds.                                | `ratings_count`, `text_reviews_count`            | Winsorize counts at p99.5 (597,244 ratings, 14,812 text reviews) before plotting or computing medians. Store capped versions alongside raw counts.                                   | Aligns visualizations and aggregates with previous cap decisions; avoids domination by mega franchises.   | Must-do now  |
| 8   | **Language-code hygiene** – currently clean, but future exports may drift.                                                                               | `language_code`                                  | Keep regex check `[A-Za-z-]+` in CI. If violations appear, normalize (`lower()`, strip); set `Unknown` when blank.                                                                   | Prevents dirty categorical labels from fragmenting dashboards.                                            | Nice-to-have |
| 9   | **Publication-year bounds** – currently all within 1800–(current year + 2).                                                                              | `publication_year`                               | Continue enforcing 1800 minimum and `current_year + 2` maximum; any violations should be reviewed and either corrected or dropped.                                                   | Maintains believable temporal trends as new data appears.                                                 | Nice-to-have |

## Implementation Roadmap

1. **Phase 04 scripts** will ingest these rules by extending `src/cleaning.py` (or a new module) to apply caps, tagging, and canonical joins.
2. **SQL parity**: replicate rules 2, 5, and 7 inside Postgres views so BI queries match pandas logic.
3. **Monitoring**: rerun the Task 02 CLI after each new data pull and diff the summary CSVs to detect regression before downstream consumers notice.

Document owner: `docs/data-cleaning-rules.md` (created 2025-12-05). Update this file whenever new anomalies surface.
